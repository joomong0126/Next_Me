import os
# protobuf í˜¸í™˜ì„±ì„ ìœ„í•œ í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ë‹¤ë¥¸ ëª¨ë“  importë³´ë‹¤ ë¨¼ì €!)
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"

try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
    print("<<<<< sqlite3 patched with pysqlite3 >>>>>")
except ImportError:
    # pysqlite3ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ sqlite3 ì‚¬ìš©
    print("<<<<< using default sqlite3 >>>>>")

print("<<<<< app.app.py IS BEING LOADED (sqlite3 patched with pysqlite3) >>>>>") # íŒ¨ì¹˜ ë‚´ìš© ëª…ì‹œ

import streamlit as st
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import LLMResult
from typing import Dict, List, Any
import time
import base64

# .env íŒŒì¼ ë¡œë“œ (í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ .env íŒŒì¼ ì°¾ê¸°)
load_dotenv(verbose=True)

st.set_page_config(page_title="ë²¡í„° DB ì±—ë´‡", page_icon=":books:", layout="wide")

st.title("ğŸ“š ë²¡í„° DB ì±—ë´‡")
st.caption("ë²¡í„° DBë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤")

# OpenAI API í‚¤ ë¡œë“œ
openai_api_key = os.getenv("OPENAI_API_KEY")

# API í‚¤ ìƒíƒœ í™•ì¸ ë° ë””ë²„ê¹… ì •ë³´
if not openai_api_key:
    st.warning("âš ï¸ .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.info("ğŸ“ .env íŒŒì¼ì„ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìƒì„±í•˜ê³  ë‹¤ìŒ ë‚´ìš©ì„ ì¶”ê°€í•˜ì„¸ìš”:")
    st.code("OPENAI_API_KEY=your_actual_api_key_here")
else:
    st.success("âœ… .env íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

# íŒŒì¼ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ (ë²¡í„° DBì—ì„œ ì¶”ì¶œí•œ ì •ë³´ ê¸°ë°˜)
def get_file_download_link(file_path, file_name):
    """íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if os.path.exists(file_path):
        with open(file_path, "rb") as f:
            bytes_data = f.read()
        b64 = base64.b64encode(bytes_data).decode()
        href = f'<a href="data:application/pdf;base64,{b64}" download="{file_name}" target="_blank">ğŸ“¥ {file_name} ë‹¤ìš´ë¡œë“œ</a>'
        return href
    else:
        return f"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_name}"

# ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì½œë°± í•¸ë“¤ëŸ¬
class StreamlitCallbackHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""
        
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text + "â–‹")  # ì»¤ì„œ íš¨ê³¼
        time.sleep(0.01)  # ìì—°ìŠ¤ëŸ¬ìš´ íƒ€ì´í•‘ íš¨ê³¼

    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        self.container.markdown(self.text)  # ìµœì¢… í…ìŠ¤íŠ¸ (ì»¤ì„œ ì œê±°)

# ë²¡í„° DB ë¡œë“œ í•¨ìˆ˜
def load_vector_database():
    """ë²¡í„° DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        if not os.path.exists("./vector_db"):
            st.error("âŒ ë²¡í„° DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.info("ğŸ“ ë¨¼ì € create_vector_db.py ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ë²¡í„° DBë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
            return None
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        
        # ë²¡í„° DB ë¡œë“œ
        vectorstore = Chroma(
            persist_directory="./vector_db",
            embedding_function=embeddings
        )
        
        st.success("âœ… ë²¡í„° DBê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return vectorstore
        
    except Exception as e:
        st.error(f"âŒ ë²¡í„° DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# RAG ì§ˆì˜ì‘ë‹µ í•¨ìˆ˜ (ì¶œì²˜ ì •ë³´ í¬í•¨)
def get_rag_response_with_sources(question, vectorstore, api_key, container):
    """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ê³  ì¶œì²˜ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        relevant_docs = retriever.get_relevant_documents(question)
        
        # ì¶œì²˜ ì •ë³´ ìˆ˜ì§‘
        sources = []
        source_files = set()
        for doc in relevant_docs:
            source_file = doc.metadata.get('source', 'Unknown')
            source_files.add(source_file)
            sources.append({
                'content': doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                'source': source_file,
                'page': doc.metadata.get('page', 'Unknown')
            })
        
        # LLM ì„¤ì •
        llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name="gpt-4o-mini",
            temperature=0
        )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        prompt_template = """
        ë‹¹ì‹ ì€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ì—¬ëŸ¬ PDF ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ëª¨ë¥¼ ê²½ìš° ì†”ì§íˆ ë§ì”€í•´ì£¼ì„¸ìš”.
        ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ì¢…í•©ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        
        ë¬¸ì„œ ë‚´ìš©:
        {context}
        
        ì§ˆë¬¸: {question}
        
        ë‹µë³€:
        """
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # RetrievalQA ì²´ì¸ ìƒì„±
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )
        
        # ì§ˆì˜ì‘ë‹µ ì‹¤í–‰
        result = qa_chain.invoke({"query": question})
        
        return result['result'], source_files, sources
        
    except Exception as e:
        error_msg = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        container.error(error_msg)
        return error_msg, set(), []

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“Š ë²¡í„° DB ìƒíƒœ")
    
    # ë²¡í„° DB ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if os.path.exists("./vector_db"):
        st.success("âœ… ë²¡í„° DBê°€ ì¡´ì¬í•©ë‹ˆë‹¤")
        st.info("ğŸ“ ì €ì¥ ìœ„ì¹˜: ./vector_db")
        
        # ë²¡í„° DB ì •ë³´ í‘œì‹œ
        try:
            embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
            vectorstore = Chroma(
                persist_directory="./vector_db",
                embedding_function=embeddings
            )
            
            # ì»¬ë ‰ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            collection = vectorstore._collection
            count = collection.count()
            st.write(f"ğŸ“Š ì €ì¥ëœ ì²­í¬ ìˆ˜: {count}")
            
        except Exception as e:
            st.warning(f"âš ï¸ ë²¡í„° DB ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
    else:
        st.error("âŒ ë²¡í„° DBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        st.info("ğŸ“ create_vector_db.py ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ë²¡í„° DBë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”")
    
    st.divider()
    st.header("âš™ï¸ ì„¤ì •")
    if not openai_api_key:
        st.error("âš ï¸ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        st.info("ğŸ“ .env íŒŒì¼ ì˜ˆì‹œ:")
        st.code("OPENAI_API_KEY=your_actual_api_key_here")
        st.info("ğŸ’¡ .env íŒŒì¼ì€ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì— ìƒì„±í•˜ì„¸ìš”.")
    else:
        st.success("âœ… OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # API í‚¤ ì¼ë¶€ë§Œ í‘œì‹œ (ë³´ì•ˆ)
        masked_key = openai_api_key[:8] + "..." + openai_api_key[-4:] if len(openai_api_key) > 12 else "***"
        st.info(f"ğŸ”‘ API í‚¤: {masked_key}")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None

if 'sources' not in st.session_state:
    st.session_state.sources = {}

# ë²¡í„° DB ë¡œë“œ (í•œ ë²ˆë§Œ ì‹¤í–‰)
if st.session_state.vectorstore is None:
    with st.spinner("ğŸ“Š ë²¡í„° DBë¥¼ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!"):
        st.session_state.vectorstore = load_vector_database()

# ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        # ì¶œì²˜ ì •ë³´ê°€ ìˆìœ¼ë©´ í‘œì‹œ
        if message["role"] == "assistant" and message.get("sources"):
            with st.expander("ğŸ“š ì°¸ê³ í•œ ë¬¸ì„œë“¤"):
                for source in message["sources"]:
                    st.write(f"**ğŸ“„ {source['source']}**")
                    st.write(f"**í˜ì´ì§€:** {source['page']}")
                    st.write(f"**ë‚´ìš©:** {source['content']}")
                    st.markdown("---")

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_question := st.chat_input(placeholder="ë²¡í„° DBì— ì €ì¥ëœ ë¬¸ì„œë“¤ì— ëŒ€í•œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”... ğŸ’¬"):
    # API í‚¤ í™•ì¸
    if not openai_api_key:
        st.error("OpenAI API í‚¤ë¥¼ ë¨¼ì € ì„¤ì •í•´ì£¼ì„¸ìš”! .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    elif st.session_state.vectorstore is None:
        st.error("ë²¡í„° DB ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    else:
        # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
        with st.chat_message("user"):
            st.markdown(user_question)
        st.session_state.messages.append({"role": "user", "content": user_question})

        # AI ë‹µë³€ ìƒì„±
        with st.chat_message("assistant"):
            # ë‹µë³€ ìƒì„±
            ai_response, source_files, sources = get_rag_response_with_sources(
                user_question, 
                st.session_state.vectorstore, 
                openai_api_key,
                st.empty()
            )
            
            # ë‹µë³€ í‘œì‹œ
            st.markdown(ai_response)
            
            # ì¶œì²˜ ì •ë³´ í‘œì‹œ
            if source_files:
                st.markdown("---")
                st.markdown("### ğŸ“š ì°¸ê³ í•œ ë¬¸ì„œë“¤")
                
                # ì¶œì²˜ íŒŒì¼ ëª©ë¡ ë° ë‹¤ìš´ë¡œë“œ
                st.markdown("**ì°¸ê³ í•œ PDF íŒŒì¼:**")
                for source_file in source_files:
                    st.write(f"ğŸ“„ {source_file}")
                    # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ ë‹¤ìš´ë¡œë“œ ë§í¬ ì œê³µ
                    if os.path.exists(source_file):
                        download_link = get_file_download_link(source_file, source_file)
                        st.markdown(download_link, unsafe_allow_html=True)
                    else:
                        st.info(f"âš ï¸ ì›ë³¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {source_file}")
                
                # ìƒì„¸ ì¶œì²˜ ì •ë³´
                with st.expander("ğŸ” ìƒì„¸ ì¶œì²˜ ì •ë³´"):
                    for i, source in enumerate(sources, 1):
                        st.markdown(f"**{i}. {source['source']}**")
                        st.write(f"**í˜ì´ì§€:** {source['page']}")
                        st.write(f"**ê´€ë ¨ ë‚´ìš©:** {source['content']}")
                        st.markdown("---")
            
        # ë©”ì‹œì§€ ì €ì¥ (ì¶œì²˜ ì •ë³´ í¬í•¨)
        st.session_state.messages.append({
            "role": "assistant", 
            "content": ai_response,
            "sources": sources
        })

# ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
    st.session_state.messages = []
    st.rerun()

# í•˜ë‹¨ì— ì‚¬ìš© íŒ ì¶”ê°€
st.divider()
with st.expander("ğŸ’¡ ì‚¬ìš© íŒ"):
    st.markdown("""
    **ì´ ì±—ë´‡ì€ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?**
    - ğŸ“Š ë²¡í„° DBì— ì €ì¥ëœ ë¬¸ì„œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤
    - ğŸ” ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤
    - ğŸ“š ë‹µë³€ì˜ ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œì‹œí•©ë‹ˆë‹¤
    - ğŸ§  ì—¬ëŸ¬ ë¬¸ì„œì˜ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤
    
    **ë” ë‚˜ì€ ë‹µë³€ì„ ìœ„í•œ íŒ:**
    - êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”
    - ë¬¸ì„œ ì† ê´€ë ¨ ì „ë¬¸ ìš©ì–´ë¥¼ ì‚¬ìš©í•´ë„ ì¢‹ìŠµë‹ˆë‹¤
    - ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì†”ì§íˆ "ëª¨ë¥¸ë‹¤"ê³  ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    - ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ì¢…í•©ì ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤
    
    **ì˜ˆì‹œ ì§ˆë¬¸:**
    - "YOI-614B POE CARD ì¸ì‹ ë¶ˆëŸ‰ì˜ ì›ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    - "ë¸”ë£¨ìŠ¤í¬ë¦° ë°œìƒ ì›ì¸ê³¼ ëŒ€ì±…ë°©ì•ˆì„ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    - "BPS-400S ìŠ¤í…ë°”ì´ 5V ë¶„ì„ ê²°ê³¼ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
    
    **ë²¡í„° DB ìƒì„±:**
    - PDF íŒŒì¼ì´ ìˆëŠ” ìƒíƒœì—ì„œ `python create_vector_db.py` ì‹¤í–‰
    - ë²¡í„° DB ìƒì„± í›„ PDF íŒŒì¼ì€ ì•ˆì „í•œ ê³³ìœ¼ë¡œ ì´ë™
    """)

# ë””ë²„ê¹…ìš© (ê°œë°œ í™˜ê²½ì—ì„œë§Œ)
if st.checkbox("ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ"):
    st.json({"ë©”ì‹œì§€ ê°œìˆ˜": len(st.session_state.messages)})
    st.json({"ë²¡í„° DB ì¡´ì¬": os.path.exists("./vector_db")})
    
    if os.path.exists("./vector_db"):
        try:
            embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
            vectorstore = Chroma(
                persist_directory="./vector_db",
                embedding_function=embeddings
            )
            collection = vectorstore._collection
            count = collection.count()
            st.json({"ë²¡í„° DB ì²­í¬ ìˆ˜": count})
        except Exception as e:
            st.json({"ë²¡í„° DB ì˜¤ë¥˜": str(e)})
    
    with st.expander("ì „ì²´ ëŒ€í™” ë‚´ì—­"):
        st.json(st.session_state.messages)